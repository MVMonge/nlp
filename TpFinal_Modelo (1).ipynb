{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzWR_S4Ee_f2"
      },
      "source": [
        "# Carga de la fuente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErJLPWbRe3iO",
        "outputId": "3377ea4e-dc96-4908-d605-a2a3b83c4638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Importación de la librería NLTK\n",
        "import nltk\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "nltk.download('cess_esp')\n",
        "nltk.download('conll2002')\n",
        "nltk.download('omw')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Definición de la locación\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Definición del origien y nombre del archivo\n",
        "root='/content/drive/MyDrive/npl_lel/fuentes/fuente_casos'\n",
        "file_name='fuente_2.txt'\n",
        "\n",
        "#Definición del corpus\n",
        "corpus=PlaintextCorpusReader(root, file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SebyiJ427BZs"
      },
      "source": [
        "# Detección de símbolos potenciales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CS5UYfM7HTD",
        "outputId": "c2668f1f-ef07-413d-8090-41e55cca7c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-zg9hy1e_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-zg9hy1e_\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.2.2)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (0.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.3.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.1.11)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.6)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2023.6.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.3)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160627 sha256=3e563baf7e5065693ec2c9f5c8d84b4245a7f6c456c0369b3ca5d9c13a6d57cb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vbnpiq5y/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.6\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.11)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-08-12 23:46:20.965884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-12 23:46:22.088651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.11)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "#Descarga e instalacion de librerías y modelos\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "\n",
        "!pip install spacy\n",
        "!spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "import pke\n",
        "import es_core_news_sm\n",
        "\n",
        "import string\n",
        "from spacy.lang.es.stop_words import STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bXx7_vhoBtM_"
      },
      "outputs": [],
      "source": [
        "#Definición del modelo spacy\n",
        "nlp=spacy.load('es_core_news_sm')\n",
        "\n",
        "#Definición de stopwords\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "puntuacion= string.punctuation\n",
        "for p in puntuacion:\n",
        "  stopwords.add(p)\n",
        "\n",
        "\n",
        "#Definición del texto\n",
        "fuente=nlp(corpus.raw())\n",
        "\n",
        "#Definición del extractor\n",
        "extractor = pke.unsupervised.TextRank()\n",
        "\n",
        "#Carga del texto\n",
        "extractor.load_document(input=fuente,language=\"es\",stoplist=stopwords,spacy_model=nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vag_dsfRDHIu"
      },
      "outputs": [],
      "source": [
        "# Definición de Keywords candidatas\n",
        "extractor.candidate_selection(pos=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZNpY2c_-CIDA"
      },
      "outputs": [],
      "source": [
        "# Construcción del grafo y ranking de los vértices\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "#Definción de la cantidad de keyphrases como T, donde T equivale a un tercio de los vértices)\n",
        "t=round(len(extractor.candidates)/3)\n",
        "keyphrases= extractor.get_n_best(t)\n",
        "\n",
        "#Defino Símbolos Potenciales\n",
        "simbolos_potenciales=[]\n",
        "for k in keyphrases:\n",
        "  simbolos_potenciales.append(k[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_potenciales)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRGpPhq2DDAd",
        "outputId": "e5264185-bb93-4575-e8b6-6578ccd0d9b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-fUDAWyUtoW"
      },
      "source": [
        "# Limpieza de símbolos potenciales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1TvKlAE_f8P7"
      },
      "outputs": [],
      "source": [
        "def get_pos(simbolo):#le doy el token  y me devuelve el pos\n",
        "  for token in fuente:\n",
        "    if str(simbolo) == token.text:\n",
        "      return token.pos_\n",
        "\n",
        "def get_dep(simbolo):#le doy el token  y me devuelve el dep\n",
        "  for token in fuente:\n",
        "    if str(simbolo) == token.text:\n",
        "      return token.dep_\n",
        "\n",
        "def is_child(parent,child):#devuelve un boolean de ser los tokens parent y child efectivamente padre e hijo\n",
        "  for token in fuente.__iter__():\n",
        "    if str(parent) == token.text:\n",
        "      for t_child in token.children:\n",
        "        if(str(child).lower()==str(t_child.text).lower()):\n",
        "          return True\n",
        "          break;\n",
        "  return False\n",
        "\n",
        "def multiple_objs(sp):#Devuelve si el simbolo tiene multiples tokens objeto\n",
        "  nro_t_objetos=0 #nro de tokens de tipo objeto en el símbolo potencial\n",
        "  for t in nlp(sp):\n",
        "    if(get_dep(t)==\"obj\"):\n",
        "      nro_t_objetos=nro_t_objetos+1\n",
        "    if(nro_t_objetos>1):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "#Quitar los símbolos que sean iguales en la lista de símbolos potenciales y dejar una sola instancia del símbolo.\n",
        "def quitar_simbolos_repetidos():\n",
        "  for sp_1 in simbolos_potenciales:\n",
        "    i=0\n",
        "    for sp_2 in simbolos_potenciales:\n",
        "      if(sp_1==sp_2):\n",
        "        i=i+1\n",
        "        if(i>1):\n",
        "          simbolos_potenciales.remove(sp_2)\n",
        "\n",
        "#Quitar los símbolos vacíos en la lista de símbolos potenciales\n",
        "def quitar_simbolos_vacios():\n",
        "  for sp in simbolos_potenciales:\n",
        "    if(sp==\"\"):\n",
        "      simbolos_potenciales.remove(sp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0wqZTr0Ondc5"
      },
      "outputs": [],
      "source": [
        "#Fase 1: detectar múltiples símbolos en keyphrases --> si hay varios los separo\n",
        "def separar_simbolos():                                                     #1. Si mi sp tiene múltiples objs en DEP entonces tengo dos símbolos.\n",
        "  list_new_symbols=[]\n",
        "  insertions = []\n",
        "  for sp in simbolos_potenciales:\n",
        "   if multiple_objs(sp):                                                    #Tengo más de un símbolo en el símbolo potencial\n",
        "      nlp_sp = nlp(sp)\n",
        "      for t in nlp_sp:                                                      #Para el token en el símbolo\n",
        "        if get_dep(t)==\"obj\" or get_dep(t)==\"iobj\" or get_dep(t)==\"dobj\":   #Si mi token es un objeto. Version anterior: SIN  or get_dep(t)=\"iobj\" or get_dep(t)=\"dobj\":\n",
        "          nuevo_simbolo=\"\"                                                  #Entonces hay un nuevo símbolo.\n",
        "          for i in nlp_sp:                                                  #Así que para todos los tokens que componen a mi símbolo\n",
        "            if is_child(t,i) or i==t:                                       # Si alguno de ellos es hijo de mi token objeto o es mi mismo token objeto\n",
        "              if nuevo_simbolo==\"\" :\n",
        "                nuevo_simbolo=i.text                                        #Entonces es parte de mi nuevo símbolo\n",
        "              else:\n",
        "                nuevo_simbolo=nuevo_simbolo+str(\" \")+i.text\n",
        "          list_new_symbols.append(nuevo_simbolo)\n",
        "      index=simbolos_potenciales.index(sp)\n",
        "      simbolos_potenciales.pop(index)                                       #Una vez que terminé de analizar todos los tokens obj de mi símbolo orignal, quito mi símbolo orignal de los símbolos potenciales\n",
        "      for i, new_symbol in enumerate(list_new_symbols):\n",
        "        insertions.append((index + i, new_symbol))\n",
        "\n",
        "  list_new_symbols.clear()\n",
        "\n",
        "  for idx, new_symbol in insertions:\n",
        "    simbolos_potenciales.insert(idx, new_symbol)\n",
        "\n",
        "  insertions.clear()\n",
        "\n",
        "  quitar_simbolos_repetidos()\n",
        "  quitar_simbolos_vacios()\n",
        "\n",
        "#Fase 2: Si el lemma del token en un símbolo es una stopword, entonces quito el token\n",
        "def remove_token(sp, token):\n",
        "    simbolo = str(sp).split()\n",
        "    nuevos_tokens = [t for t in simbolo if t != token]\n",
        "    simbolo_actualizado = ' '.join(nuevos_tokens)\n",
        "    return simbolo_actualizado\n",
        "\n",
        "def is_lemma_stopword(t):# Si el lemma del token en un símbolo es una stopword, entonces quito el token\n",
        "  nlp = spacy.load('es_core_news_sm')\n",
        "  doc = nlp(t)\n",
        "  if doc[0].lemma_ in stopwords:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def quitar_simbolos_stopword():\n",
        "  simbolos_a_quitar=[]\n",
        "  for sp in simbolos_potenciales:\n",
        "    nlp_sp = nlp(sp)\n",
        "    for t in nlp_sp:                                                                            #Para el token en el símbolo\n",
        "      if is_lemma_stopword(str(t)):\n",
        "        simbolos_potenciales[simbolos_potenciales.index(sp)]=remove_token(str(sp),str(t))\n",
        "        if str(sp)==str(t):                                                                             #Si token es todo el símbolo, entonces quito el símbolo\n",
        "          sp=remove_token(str(sp),str(t))\n",
        "          simbolos_potenciales.remove(sp)\n",
        "        else:\n",
        "          sp=remove_token(str(sp),str(t))\n",
        "  quitar_simbolos_repetidos()\n",
        "  quitar_simbolos_vacios()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wWjKz-MGLxvs"
      },
      "outputs": [],
      "source": [
        "separar_simbolos()\n",
        "quitar_simbolos_stopword()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(simbolos_potenciales)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZaH34e50HAc",
        "outputId": "131f8f91-734c-471c-fe7e-4ff40235417a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['columna “ autoevaluación ”', 'columna “ evaluación ”', 'enviarles email informándoles', 'empleados asociados', 'supervisores', 'supervisores asociados', 'estándar smart', 'explicado', 'información', 'feedback', 'autoevaluación vencido', 'cargo asociado', 'objetivos asociados', 'cargo', 'periodo', 'empleados registrados', 'autoevaluación', 'información previa', 'fecha mínima', 'fecha máxima', 'competencias específicas', 'feedback vencido', 'supervisión vencido', 'empleado seleccionado', 'objetivos específicos', 'texto libre', 'tiempos establecidos', 'tarea identifica', 'única', 'configuración iniciales', 'link provisto', 'proceso integral', 'aspectos generales', 'columna feedback', 'actividades', 'supervisión', 'competencias genéricas', 'supervisor detectado', 'asociado', 'evaluación completo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_potenciales)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Z_DaAbmQ7r",
        "outputId": "bd0abb41-45c3-473b-adc9-1abda93bf065"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duyxIrpi6lgE"
      },
      "source": [
        "# Iniciación y nombramiento  de los símbolos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rq8oDjcK5_xF"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Definición de la clase símbolo\n",
        "class Simbolo:\n",
        "  nombre=\"\"\n",
        "  clase=\"\"\n",
        "  nocion=[]\n",
        "  impacto=[]\n",
        "  otros_nombres=[]\n",
        "  especializaciones=[]\n",
        "  es_referenciado=False\n",
        "  hace_referencia=False\n",
        "\n",
        "  def __init__(self, nombre):\n",
        "    self.nombre = nombre\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Pq_UsjQw7aC5"
      },
      "outputs": [],
      "source": [
        "simbolos_confirmados= []\n",
        "for simbolo in simbolos_potenciales:\n",
        "  simbolos_confirmados.append(\n",
        "      Simbolo(simbolo)\n",
        "  )\n",
        "\n",
        "del(simbolos_potenciales)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_confirmados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jab-sQ0zrv39",
        "outputId": "2057f355-8614-42bf-bdb7-8566721c0b22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syg8_z-07ZTw"
      },
      "source": [
        "# Clasificación de los símbolos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zTWZ7Mx5EOCm"
      },
      "outputs": [],
      "source": [
        "def symbol_has_root(nlp_sc):#Devuelve un bool especificando si el símbolo tiene un token ROOT\n",
        "  for t in nlp_sc:\n",
        "    if t.dep_==\"ROOT\":\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def symbol_get_root(nlp_sc): #Devuelve el token ROOT del símbolo.\n",
        "  for t in nlp_sc:\n",
        "    if t.dep_==\"ROOT\":\n",
        "      return t\n",
        "  return None\n",
        "\n",
        "def is_named_entity(sc): #Devuelve un bool especificando si el símbolo es una de las Named Entities en la fuente\n",
        "  named_entities = [(entity.text) for entity in fuente.ents]\n",
        "  for n_e in named_entities:\n",
        "    if sc.nombre.lower()==n_e.lower():\n",
        "      return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Be2HMs4XCz0I"
      },
      "outputs": [],
      "source": [
        "def clasificar_token_pos(pos_tag):#Define cómo clasifico a un símbolo en base a sus POS tags:\n",
        "  if pos_tag==None:\n",
        "    return None\n",
        "  else:\n",
        "    if pos_tag.startswith(\"VERB\"):\n",
        "      return \"VERBO\"\n",
        "    elif pos_tag.startswith(\"NOUN\"):\n",
        "      return \"SUJETO\"\n",
        "    elif pos_tag in [\"PRON\", \"DET\"]:\n",
        "      return \"OBJETO\"\n",
        "    elif pos_tag.startswith(\"ADJ\"):\n",
        "      return \"ESTADO\"\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def clasificar_token_dep(dep_tag):                                  #Define cómo clasifico a un símbolo en base a sus DEP tags:\n",
        "  if dep_tag in [\"ROOT\", \"advcl\", \"ccomp\"]:\n",
        "    return \"VERBO\"\n",
        "  elif dep_tag in [\"nsubj\", \"nsubjpass\"]:\n",
        "    return \"SUJETO\"\n",
        "  elif dep_tag in [\"obj\",\"dobj\", \"iobj\", \"pobj\", \"attr\"]:\n",
        "    return \"OBJETO\"\n",
        "  elif dep_tag in [\"amod\",\"advmod\"]:\n",
        "    return \"ESTADO\"\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hVBP4B7JP6jb"
      },
      "outputs": [],
      "source": [
        "#Intenta determinar la clase del símbolo en base a la DEP tag asignado al token root en la fuente\n",
        "#Si esta es nula, intenta determinar la clase del símbolo en base a la DEP tag asignado al token root en el contexto del símbolo\n",
        "def definir_clase_por_dep(dep_tag,root):\n",
        "  clase=clasificar_token_dep(dep_tag)\n",
        "  if clase==None:\n",
        "    clase=clasificar_token_dep(root.dep_)\n",
        "  return clase\n",
        "\n",
        "#Intenta determinar la clase del símbolo en base a la POS tag asignado al token root en la fuente\n",
        "#Si esta es nula, intenta determinar la clase del símbolo en base a la POS tag asignado al token root en el contexto del símbolo\n",
        "def definir_clase_por_pos(pos_tag,root):\n",
        "  clase=clasificar_token_pos(pos_tag)\n",
        "  if clase==None:\n",
        "    clase=clasificar_token_pos(root.pos_)\n",
        "  return clase\n",
        "\n",
        "#Determina la clase del símbolo\n",
        "def definir_clase(sc,nlp_sc):\n",
        "  clase=None\n",
        "  if is_named_entity(sc): #if sc es una Named entity, entonces mi símbolo es un SUJETO\n",
        "    clase=\"SUJETO\"\n",
        "  if symbol_has_root(nlp_sc):\n",
        "    root=symbol_get_root(nlp_sc)\n",
        "    clase_dep=definir_clase_por_dep(get_dep(root),root)\n",
        "    clase_pos=definir_clase_por_pos(get_pos(root),root)\n",
        "    if clase_dep==clase_pos:\n",
        "      clase=clase_dep# adicionar if clase=None en la llamada de la funcion\n",
        "    elif clase_dep==None:\n",
        "      clase=clase_pos\n",
        "    elif clase_pos==None:\n",
        "      clase=clase_dep\n",
        "    elif clase_dep==\"VERBO\":\n",
        "      if clase_pos==\"SUJETO\" or clase_pos==\"OBJETO\":\n",
        "        clase=\"OBJETO\"\n",
        "      else:\n",
        "        clase=clase_pos\n",
        "    elif clase_dep==\"OBJETO\" or clase_dep==\"SUJETO\" or clase_dep==\"ESTADO\":\n",
        "      clase=clase_dep\n",
        "    if clase==None:\n",
        "      print(\"SM\",nlp_sc.text,\"CLASE_DEP: \",clase_dep,\"CLASE_POS: \",clase_pos)\n",
        "  sc.clase=clase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Llama a determinar la clase del símbolo.\n",
        "#Si la clase del símbolo resulta ser nula, entonces quito el símbolo de la lista de símbolos confirmados\n",
        "#\n",
        "for sc in simbolos_confirmados:\n",
        "  nlp_sc=nlp(sc.nombre)\n",
        "  definir_clase(sc,nlp_sc)\n",
        "  if sc.clase==None: # Una vez clasificado, quito el simbolo si no aplican\n",
        "    simbolos_confirmados.remove(sc)"
      ],
      "metadata": {
        "id": "ywihLsRMtNtF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_confirmados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YtsTT90p3nx",
        "outputId": "a8bc4305-afaa-48e0-8c30-a5222788f03f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF2KBf6iTesP"
      },
      "source": [
        "# Noción e Impacto de los símbolos\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sent(sc_name):                          #Devuelve una lista de oraciones que incluyen al símbolo parámetro\n",
        "  sentences=[]\n",
        "  for sent in fuente.sents:\n",
        "    if sc_name in sent.text.lower():\n",
        "      sentences.append(sent.text)\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "u38Ogbk6ziXl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uXYAdXCzFmla"
      },
      "outputs": [],
      "source": [
        "def quitar_oraciones_repetidas(sentences):      #De encontrarlas, quitas las instancias repetidas de una frase\n",
        "  for sent_1 in sentences:\n",
        "    i=0\n",
        "    for sent_2 in sentences:\n",
        "      if(sent_1==sent_2):\n",
        "        i=i+1\n",
        "        if(i>1):\n",
        "          sentences.remove(sent_2)\n",
        "  return sentences\n",
        "\n",
        "def describir_simbolo(simbolo):\n",
        "  sentences=get_sent(simbolo.nombre)# Oraciones que contengan el nombre del símbolo\n",
        "  nocion = []\n",
        "  impacto = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    nlp_sc=nlp(simbolo.nombre.lower())\n",
        "    root=symbol_get_root(nlp_sc)\n",
        "    for token in doc:\n",
        "      if str(token.text.lower()) == str(root):\n",
        "        if clasificar_token_dep(token.dep_) == \"SUJETO\":\n",
        "          nocion.append(sentence)\n",
        "        elif clasificar_token_dep(token.dep_) == \"OBJETO\":\n",
        "          impacto.append(sentence)\n",
        "        elif clasificar_token_dep(token.dep_) == \"ESTADO\":\n",
        "          nocion.append(sentence)\n",
        "        elif clasificar_token_dep(token.dep_) == \"VERBO\":\n",
        "          impacto.append(sentence)\n",
        "        else:\n",
        "          impacto.append(sentence)\n",
        "\n",
        "  nocion=quitar_oraciones_repetidas(nocion) #De encontrarlas, quitas las instancias repetidas de una frase\n",
        "  impacto=quitar_oraciones_repetidas(impacto) #De encontrarlas, quitas las instancias repetidas de una frase\n",
        "  simbolo.nocion=nocion\n",
        "  simbolo.impacto=impacto\n",
        "\n",
        "def describir_simbolos_confirmados():\n",
        "  for sc in simbolos_confirmados:\n",
        "    describir_simbolo(sc)\n",
        "  for sc in simbolos_confirmados: #Quitar símbolos con nociones e impactos vacios\n",
        "    if len(sc.nocion)==0:\n",
        "      if len(sc.impacto)==0:\n",
        "        simbolos_confirmados.remove(sc)\n",
        "\n",
        "describir_simbolos_confirmados()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_confirmados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiApZ4hw_8Ln",
        "outputId": "a9031f31-d19b-4cc0-f611-98b621883b00"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2pKTpmZTp-g"
      },
      "source": [
        "# Referenciacion entre símbolos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "A6gHEHOO7gE7"
      },
      "outputs": [],
      "source": [
        "#1. Subraya los casos de menciones en otros símbolos. #. Si hay un símbolo con 0 menciones. Quitarlo del LEL final.\n",
        "\n",
        "def marcar_referencias(simbolo):  #Marca las referencias del simbolo en otros simbolos, en estos otros simbolos donde se hacen referencias marca el atributo hace_referencia como true.\n",
        "#Devuelve un boolean true si se marcaron referencias al símbolo en otros símbolos, y false si no se encontraron referencias.\n",
        "  es_referenciado=False\n",
        "  nombre=simbolo.nombre\n",
        "  for sc in simbolos_confirmados:\n",
        "    if nombre!=sc.nombre: #mientras no se trate del mismo simbolo\n",
        "      for noc in sc.nocion:\n",
        "        if nombre in noc:\n",
        "          sc.nocion[sc.nocion.index(noc)]= noc.replace(nombre, \"\\033[4m\" + nombre + \"\\033[0m\")\n",
        "          es_referenciado=True #mi simbolo es referenciado por otro\n",
        "          sc.hace_referencia=True #y el otro simbolo hace refrencias\n",
        "      for imp in sc.impacto:\n",
        "        if nombre in imp:\n",
        "          sc.impacto[sc.impacto.index(imp)]=imp.replace(nombre, \"\\033[4m\" + nombre + \"\\033[0m\")\n",
        "          es_referenciado=True\n",
        "          sc.hace_referencia=True\n",
        "  return es_referenciado\n",
        "\n",
        "def quitar_simbolos_aislados():                            #Quita los símbolos que no hacen referencias o no son referenciados por otros símbolos.\n",
        "  for sc in simbolos_confirmados:\n",
        "    if sc.es_referenciado==False:\n",
        "      simbolos_confirmados.remove(sc)\n",
        "    else:\n",
        "      if sc.hace_referencia==False:\n",
        "        simbolos_confirmados.remove(sc)\n",
        "\n",
        "def referenciacion_de_simbolos():                       #Define el proceso de referencias entre los símbolos\n",
        "  for sc in simbolos_confirmados:                       #Marco todas las referencias entre los simbolos\n",
        "    sc.es_referenciado=marcar_referencias(sc)\n",
        "  quitar_simbolos_aislados()                            #Una vez marcadas las referencias quito los símbolos que no tienen relacion con el resto del LEL (es decir, estan aislados)\n",
        "\n",
        "referenciacion_de_simbolos()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_confirmados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqkRBf_Cc7SL",
        "outputId": "d075cb85-8795-4d5b-a866-ac24e3342319"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Simbolos sinonimos\n"
      ],
      "metadata": {
        "id": "PKQGC8VagOED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Revisar si hay algunos símbolos que sean sinónimos\n",
        "\n",
        "def asimilar_sinonimos():                                                                         #Si dos simbolos tienen igual clase, nocion e impacto los considero sinónimos y los vuelvo un solo símbolo\n",
        "  for sc_1 in simbolos_confirmados:\n",
        "    sinonimos=[]\n",
        "    for sc_2 in simbolos_confirmados:\n",
        "      if sc_1.nombre!=sc_2.nombre:\n",
        "        if sc_1.clase==sc_2.clase and sc_1.nocion==sc_2.nocion and sc_1.impacto==sc_2.impacto:    #Son sinonimos\n",
        "          sinonimos.append(str(sc_2.nombre))\n",
        "          simbolos_confirmados.remove(sc_2)                                                       #Quito uno de los símbolos sinónimos de la lista de simbolos.\n",
        "    sc_1.otros_nombres= sinonimos                                                                 #SUmo los otros nombres del símbolo\n",
        "\n",
        "asimilar_sinonimos()"
      ],
      "metadata": {
        "id": "y5zzLkStbxY4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_confirmados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bt5D_nGip0Y",
        "outputId": "3cfb1983-6efa-4fed-f48e-1f92caadf563"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jerarquías entre simbolos"
      ],
      "metadata": {
        "id": "TyBfBBcBkZQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def es_especializacion(simbolo, especializacion):#Devuelve un boolean de si el parametro especializacion es o no una especializacion del parámetro símbolo# Si el nombre o algún nombre sinónimo de sc_1 es igual a la root de otros símbolos entonces sc_2 es una especialización (\"hijo\") de sc_1#Armo dos listas con los nombres de cada simbolo.\n",
        "  simbolo_nombres=[]\n",
        "  simbolo_nombres.append(simbolo.nombre)\n",
        "  especializacion_nombres=[]\n",
        "  especializacion_nombres.append(especializacion.nombre)\n",
        "  for otro_nombre in simbolo.otros_nombres:\n",
        "    simbolo_nombres.append(otro_nombre)\n",
        "  for otro_nombre in especializacion.otros_nombres:\n",
        "    especializacion_nombres.append(otro_nombre)    #Checkeo si alguno de los nombres de la especializacion tienen como root a alguno de los nombre del simbolos\n",
        "  for esp in especializacion_nombres:\n",
        "    if str(symbol_get_root(nlp(esp))) in simbolo_nombres: #Si si, entonces devuelvo TRUE: es una especializacion\n",
        "      return True\n",
        "  return False #Si no, entonces devuelvo FALSE: no es una especialización\n",
        "\n",
        "#Si un símbolo es la root de otros símbolos--> están en una jerarquía taxonómica.\n",
        "def definir_jerarquias():\n",
        "  for sc_1 in simbolos_confirmados:\n",
        "    esps=[]\n",
        "    for sc_2 in simbolos_confirmados:\n",
        "      if str(sc_1.nombre)!=str(sc_2.nombre):    #Estoy haciendo un loop anidado, asi que si es mismo simbolo entonces no lo evaluo\n",
        "        # Si el nombre o algún nombre sinónimo de sc_1 es igual a la root de otros símbolos\n",
        "        # entonces sc_2 es una especialización (\"hijo\") de sc_1\n",
        "        if es_especializacion(sc_1,sc_2): #Si sc_2 es una especializacion de sc_1, estan en una jerarquia taxonómica, y sumo a sc_2 como especializcion de sc_1\n",
        "          esps.append(sc_2)\n",
        "    sc_1.especializaciones=esps\n",
        "\n",
        "definir_jerarquias()\n",
        "\n"
      ],
      "metadata": {
        "id": "J8nngbNGijkn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(simbolos_confirmados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKIOYmXB8go9",
        "outputId": "e9e87cc7-fc1a-4531-d3fd-d3afe1d1d3eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6iduQ2SxCyr"
      },
      "source": [
        "# Configuración de los nombres\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reglas de configuración de los nombres, en base a Proceso de Creación del LEL :\n",
        "# Regla 1. Debe utilizarse la forma singular el nombre del símbolo, excepto en casos en que la versión singular y la plural no se usan con el mismo significado\n",
        "# o si la forma singular no tiene sentido en el contexto de la aplicación.\n",
        "#Regla 2: Hay que sacar los que hayan quedado mal configurados y poner los en infinitivo a los verbos.\n",
        "\n",
        "#En ambos casos, es decir tanto para que una palabra quede en singular como en infinitivo, la palabra debería ser igual a su lemma.\n",
        "#En el caso de los símbolos \"ESTADO\", esto no debería aplicar.\n",
        "\n",
        "def es_lemma(nombre):\n",
        "  if nombre.text==nombre.lemma_:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def configurar_nombres():\n",
        "  for sc in simbolos_confirmados:\n",
        "    if sc.clase!=\"ESTADO\":\n",
        "      nombre=nlp(sc.nombre)\n",
        "      for token in nombre:                                        #Para cada token que compone al nombre checkeo si es igual a su lemma\n",
        "        if es_lemma(token)==False:\n",
        "          sc.nombre=sc.nombre.replace(token.text,token.lemma_)\n",
        "\n",
        "configurar_nombres()\n",
        "\n"
      ],
      "metadata": {
        "id": "3sfeKCsh-4cj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simbolos Homónimos"
      ],
      "metadata": {
        "id": "XZSfwGutCvse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Una vez reconfigurados los nombres de los símbolos, si dos símbolos quedaron con el mismo nombre,\n",
        "#pero como ya pasada la instancia de chequeo de sinónimos sé que efectivamente no se trata de sinónimos porque no coinciden en clase o descripción\n",
        "#Entonces considero a estos símbolos como símbolos homonimos."
      ],
      "metadata": {
        "id": "zjllOHHxC27H"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTMQ_dxo57E7"
      },
      "source": [
        "# Impresión de LEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def get_simbolo_string(simbolo):\n",
        "    output = \"\\nNOMBRE: \" + simbolo.nombre + \"\\n\"\n",
        "    if simbolo.otros_nombres:\n",
        "        output += \"Otros nombres:\\n\"\n",
        "        for nom in simbolo.otros_nombres:\n",
        "            output += \"- \" + nom + \"\\n\"\n",
        "    output += \"CLASE: \" + simbolo.clase + \"\\n\"\n",
        "    output += \"NOCIÓN:\\n\"\n",
        "    for noc in simbolo.nocion:\n",
        "        output += \"\\t- \" + noc + \"\\n\"\n",
        "    output += \"IMPACTO:\\n\"\n",
        "    for imp in simbolo.impacto:\n",
        "        output += \"\\t- \" + imp + \"\\n\"\n",
        "    if simbolo.especializaciones:\n",
        "        output += \"Especializaciones:\\n\"\n",
        "        for esp in simbolo.especializaciones:\n",
        "            output += esp.nombre + \"\\n\"\n",
        "    return output\n",
        "\n",
        "def imprimir_lel():\n",
        "    output = \"\"\n",
        "    for sc in simbolos_confirmados:\n",
        "        output += get_simbolo_string(sc)\n",
        "    return output\n",
        "\n",
        "output= imprimir_lel().replace(\"\\x1b[4m\",\"\")\n",
        "output=output.replace(\"\\x1b[0m\",\"\")\n",
        "with open('output.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(output)\n",
        "files.download('output.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gCWnhH59-xJ8",
        "outputId": "0148f529-e2ca-4247-9ac2-e46af91d30d7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1dfa2e69-0056-43da-bd58-defd3262e82c\", \"output.txt\", 18784)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bzWR_S4Ee_f2",
        "SebyiJ427BZs",
        "I-fUDAWyUtoW",
        "duyxIrpi6lgE",
        "syg8_z-07ZTw",
        "d6iduQ2SxCyr"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}